View(alpha_final)
plot(df %>% select(COUMESTROL, alpha_invsimpson))
####### TESTING FOR DIVERSITY ######
test = cor(df_heatmap,method="spearman")
pheatmap(cor(na.omit(df_heatmap),method="spearman"),fontsize=3)
View(test)
mdro_binary = df$mdro > 1
df$mdro_binary = mdro_binary
tmp = df[,c("alpha_invsimpson","FPED_D_YOGURT")]
tmp = tmp %>% mutate(diversity = .[[1]], quartile = ntile(.[[2]], 4))
#View(tmp)
tmp$quartile = as.factor(tmp$quartile)
#tmp$diversity = as.factor(tmp$diversity)
model = glm(diversity~quartile, data=tmp)
summary(model)
####### TESTING FOR MDRO ######
tmp = df[,c("vre","alpha_invsimpson")]
tmp = tmp %>% mutate(diversity = .[[1]], quartile = ntile(.[[2]], 4))
#View(tmp)
tmp$quartile = as.factor(tmp$quartile)
tmp$diversity = as.factor(tmp$diversity)
model = glm(diversity~quartile, data=tmp, family=binomial)
summary(model)
##########
model = glm(mdro_binary~TOTAL_DIETARY_FIBER, data=df, family=binomial)
summary(model)
model = glm(mdro_binary~RIBOFLAVIN_VITAMIN_B2, data=df)
summary(model)
if ((0 < quartile_listed[1,"Estimate"]) &
(quartile_listed[1,"Estimate"] < quartile_listed[2,"Estimate"]) &
(quartile_listed[2,"Estimate"] < quartile_listed[3,"Estimate"]) &
(quartile_listed[3,4] < 0.05)) {
quartile_list[["increased"]][[colnames(df)[i]]] = quartile_listed
}
quartile_list[["increased"]][[colnames(df)[i]]] = quartile_listed
####### DIVERSITY ########
final = final_quartile = data.frame()
quartile_list = list()
for (i in 1:ncol(df)) {
tmp = df %>% select(i,"alpha_invsimpson")
tmp_quartile = tmp %>% select(1) %>% mutate(quartile = as.factor(ntile(., 4))) %>% select(quartile)
tmp = cbind(tmp, tmp_quartile)
formula = formula(paste("alpha_invsimpson~", colnames(df)[i]))
model = glm(formula, data=tmp)
tab = summary(model)
tab_coef = tab$coefficients
#tab_coef = tab_coef[-1,]
tab_coef_backup = tab_coef
rownames(tab_coef) = paste0("cont", ".", rownames(tab_coef))
if (nrow(tab_coef) < 2) {
next
}
final = rbind(final, tab_coef)
model_quartile = glm(alpha_invsimpson~quartile, data=tmp)
tab_quartile = summary(model_quartile)
tab_coef_quartile = tab_quartile$coefficients
quartile_listed = tab_coef_quartile[-1,]
if ((0 < quartile_listed[1,"Estimate"]) &
(quartile_listed[1,"Estimate"] < quartile_listed[2,"Estimate"]) &
(quartile_listed[2,"Estimate"] < quartile_listed[3,"Estimate"]) &
(quartile_listed[3,4] < 0.05)) {
quartile_list[["increased"]][[colnames(df)[i]]] = quartile_listed
}
if ((0 > quartile_listed[1,"Estimate"]) &
(quartile_listed[1,"Estimate"] > quartile_listed[2,"Estimate"]) &
(quartile_listed[2,"Estimate"] > quartile_listed[3,"Estimate"]) &
(quartile_listed[1,4] < 0.05)) {
quartile_list[["decreased"]][[colnames(df)[i]]] = quartile_listed
}
#tab_coef_quartile = tab_coef_quartile[-1,]
rownames(tab_coef_quartile) = paste0("quartile", c(1:4), ".", rownames(tab_coef_backup))
final = rbind(final, tab_coef_quartile)
}
alpha_final = final %>% filter(!grepl('Intercept', rownames(.))) %>%
filter(!grepl('quartile.', rownames(.))) %>%
arrange(4) %>% filter(`Pr(>|t|)` < 0.05)
quartile_listed
quartile_list
####### DIVERSITY ########
final = final_quartile = data.frame()
quartile_list = list()
for (i in 1:ncol(df)) {
tmp = df %>% select(i,"alpha_invsimpson")
tmp_quartile = tmp %>% select(1) %>% mutate(quartile = as.factor(ntile(., 4))) %>% select(quartile)
tmp = cbind(tmp, tmp_quartile)
formula = formula(paste("alpha_invsimpson~", colnames(df)[i]))
model = glm(formula, data=tmp)
tab = summary(model)
tab_coef = tab$coefficients
#tab_coef = tab_coef[-1,]
tab_coef_backup = tab_coef
rownames(tab_coef) = paste0("cont", ".", rownames(tab_coef))
if (nrow(tab_coef) < 2) {
next
}
final = rbind(final, tab_coef)
model_quartile = glm(alpha_invsimpson~quartile, data=tmp)
tab_quartile = summary(model_quartile)
tab_coef_quartile = tab_quartile$coefficients
quartile_listed = tab_coef_quartile[-1,]
if ((0 < quartile_listed[1,"Estimate"]) &
(quartile_listed[1,"Estimate"] < quartile_listed[2,"Estimate"]) &
(quartile_listed[2,"Estimate"] < quartile_listed[3,"Estimate"]) &
(quartile_listed[3,4] < 0.05)) {
quartile_list[["increased"]][[colnames(df)[i]]] = quartile_listed
}
if ((0 > quartile_listed[1,"Estimate"]) &
(quartile_listed[1,"Estimate"] < quartile_listed[2,"Estimate"]) &
(quartile_listed[2,"Estimate"] < quartile_listed[3,"Estimate"]) &
(quartile_listed[1,4] < 0.05)) {
quartile_list[["decreased"]][[colnames(df)[i]]] = quartile_listed
}
#tab_coef_quartile = tab_coef_quartile[-1,]
rownames(tab_coef_quartile) = paste0("quartile", c(1:4), ".", rownames(tab_coef_backup))
final = rbind(final, tab_coef_quartile)
}
quartile_list$decreased
quartile_listed
### Install packages only if not present
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("forestplot")) install.packages("forestplot")
if (!require("ggmap")) install.packages("ggmap")
if (!require("maps")) install.packages("maps")
if (!require("mapdata")) install.packages("mapdata")
### Loading packages
library(ggpubr); library(tidyverse); library(forestplot); library(maps); library(mapdata); library(ggmap); library(tidyverse)
### Setting working directory for Github repo
setwd('~/Desktop/mask_export')
### Setting working directory for Github repo
setwd('~/Documents/GitHub/ProjectSnafu/')
# SECTION 1: Generate Price Index AND combine into observation table ####
ppe_meta = read.csv("InputData/20-0703-ppe_meta_data.csv", stringsAsFactors = FALSE)
ppe_meta = ppe_meta %>% filter(type=="grocery")
str(ppe_meta)
ppe_meta$price_of_good = as.numeric(ppe_meta$price_of_good)
AvgZscorePriceIndex = ppe_meta %>%
filter(!(name_of_good == "oranges")) %>%
group_by(name_of_good) %>%
mutate(zscore_price = (price_of_good-mean(price_of_good, na.rm = TRUE)) / sd(price_of_good, na.rm = TRUE)) %>%
group_by(id) %>%
summarize(avg_zscore_price_index = mean(zscore_price, na.rm = TRUE),
median_zscore_price_index = median(zscore_price, na.rm = TRUE)) %>%
left_join(ppe_meta, by = "id") %>%
select(id = id, county = county, name = name, type = type, location = location, avg_zscore_price_index, median_zscore_price_index, pop_total) %>%
unique()
# Making combined file of ppe and avg price
small_PI_table = AvgZscorePriceIndex %>%
select(id, avg_zscore_price_index, median_zscore_price_index, pop_total)
ppe_obs = read.csv("InputData/20-0703-ppe_observation_data.csv", stringsAsFactors = FALSE) %>%
filter(type=="grocery") %>%
left_join(small_PI_table, by = "id")
# Convert blank observations to zero
ppe_obs[is.na(ppe_obs)] = 0
merged = ppe_obs
merged$age = replace(merged$age, merged$age=="A", "a")
merged$age = replace(merged$age, merged$age=="E", "e")
merged$age = replace(merged$age, merged$age=="Y", "y")
merged$age = replace(merged$age, merged$age=="C", "c")
# Filter out rows with the non-standard typo notation of ages
merged = merged %>% filter(age %in% c("a","e","y","c"))
old_age_to_new_age_mapping = data.frame(age = c("a", "e", "y", "c", "t"),
age_two_bins = c("30+", "30+", "0-29", "0-29", "0-29")) # 2-bin age not used for analysis
merged = merged %>% left_join(old_age_to_new_age_mapping, by = "age")
hist(merged$avg_zscore_price_index)
case_prevalence_map = ppe_meta %>% select(id, case_rate, no_cases) %>% unique()
merged = merged %>% left_join(case_prevalence_map, by = "id")
#merged$pop_total <- merged$pop_total/10000
#merged$case_rate <- merged$case_rate/10
write.csv(merged, "InputData/meta_merged_observed.csv")
# SECTION 2: Generate TABLE 1 ####
table1_names = merged %>% group_by(county) %>% summarize(n = n(), mask_usage = paste0(c(sum(mask),"/", n(), " (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
unite(name, county:n, sep = ", n = ", remove = FALSE)
table1_names
table1_gender = merged %>% group_by(county, gender) %>% summarize(mask_usage = paste0(c(sum(mask), "/", n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = gender, values_from = mask_usage)
table1_gender
table1_age = merged %>% group_by(county, age) %>% summarize(mask_usage = paste0(c(sum(mask), "/", n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = age, values_from = mask_usage)
table1_age
table1_price_index = merged %>% group_by(county) %>% summarize(max_PI = round(max(avg_zscore_price_index, na.rm = TRUE), 1),
min_PI = round(min(avg_zscore_price_index, na.rm = TRUE), 1)) %>% unite(range, min_PI:max_PI, sep = "," )
table1_price_index
table1_case_rate = merged %>% select(county, case_rate, no_cases) %>%
unique()
table1_case_rate
table1_case_rate$combined = paste0(table1_case_rate$case_rate, " (", table1_case_rate$no_cases, ")")
table1_case_rate = table1_case_rate %>% select(county, combined) %>% # note that i took the first occurence of dane, so the higher case rate
arrange(county, desc(combined)) %>% distinct(county, .keep_all=TRUE)
table1_case_rate
## Finally, make a total row for bottom of table, as 1 row dataframe
total = cbind("Wisconsin", "Total", merged %>%
summarize(n = n(), mask_usage = paste0(c(sum(mask),"/",n(), " (", round(sum(mask)/n()*100, 1), ")"), collapse = "")),
merged %>% group_by(gender) %>% summarize(mask_usage = paste0(c(sum(mask), "/",n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = gender, values_from = mask_usage),
merged %>% group_by(age) %>% summarize(mask_usage = paste0(c(sum(mask), "/",n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = age, values_from = mask_usage),
0,0)
ncol(total)
### Install packages only if not present
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("forestplot")) install.packages("forestplot")
if (!require("ggmap")) install.packages("ggmap")
if (!require("maps")) install.packages("maps")
if (!require("mapdata")) install.packages("mapdata")
### Loading packages
library(ggpubr); library(tidyverse); library(forestplot); library(maps); library(mapdata); library(ggmap); library(tidyverse)
### Setting working directory for Github repo
setwd('~/Documents/GitHub/ProjectSnafu/')
# SECTION 1: Generate Price Index AND combine into observation table ####
ppe_meta = read.csv("InputData/20-0708-ppe_meta_data.csv", stringsAsFactors = FALSE)
ppe_meta = ppe_meta %>% filter(type=="grocery")
str(ppe_meta)
ppe_meta$price_of_good = as.numeric(ppe_meta$price_of_good)
AvgZscorePriceIndex = ppe_meta %>%
filter(!(name_of_good == "oranges")) %>%
group_by(name_of_good) %>%
mutate(zscore_price = (price_of_good-mean(price_of_good, na.rm = TRUE)) / sd(price_of_good, na.rm = TRUE)) %>%
group_by(id) %>%
summarize(avg_zscore_price_index = mean(zscore_price, na.rm = TRUE),
median_zscore_price_index = median(zscore_price, na.rm = TRUE)) %>%
left_join(ppe_meta, by = "id") %>%
select(id = id, county = county, name = name, type = type, location = location, avg_zscore_price_index, median_zscore_price_index, pop_total) %>%
unique()
# Making combined file of ppe and avg price
small_PI_table = AvgZscorePriceIndex %>%
select(id, avg_zscore_price_index, median_zscore_price_index, pop_total)
ppe_obs = read.csv("InputData/20-0708-ppe_observation_data.csv", stringsAsFactors = FALSE) %>%
filter(type=="grocery") %>%
left_join(small_PI_table, by = "id")
# Convert blank observations to zero
ppe_obs[is.na(ppe_obs)] = 0
merged = ppe_obs
merged$age = replace(merged$age, merged$age=="A", "a")
merged$age = replace(merged$age, merged$age=="E", "e")
merged$age = replace(merged$age, merged$age=="Y", "y")
merged$age = replace(merged$age, merged$age=="C", "c")
# Filter out rows with the non-standard typo notation of ages
merged = merged %>% filter(age %in% c("a","e","y","c"))
old_age_to_new_age_mapping = data.frame(age = c("a", "e", "y", "c", "t"),
age_two_bins = c("30+", "30+", "0-29", "0-29", "0-29")) # 2-bin age not used for analysis
merged = merged %>% left_join(old_age_to_new_age_mapping, by = "age")
hist(merged$avg_zscore_price_index)
case_prevalence_map = ppe_meta %>% select(id, case_rate, no_cases) %>% unique()
merged = merged %>% left_join(case_prevalence_map, by = "id")
#merged$pop_total <- merged$pop_total/10000
#merged$case_rate <- merged$case_rate/10
write.csv(merged, "InputData/meta_merged_observed.csv")
# SECTION 2: Generate TABLE 1 ####
table1_names = merged %>% group_by(county) %>% summarize(n = n(), mask_usage = paste0(c(sum(mask),"/", n(), " (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
unite(name, county:n, sep = ", n = ", remove = FALSE)
table1_names
table1_gender = merged %>% group_by(county, gender) %>% summarize(mask_usage = paste0(c(sum(mask), "/", n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = gender, values_from = mask_usage)
table1_gender
table1_age = merged %>% group_by(county, age) %>% summarize(mask_usage = paste0(c(sum(mask), "/", n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = age, values_from = mask_usage)
table1_age
table1_price_index = merged %>% group_by(county) %>% summarize(max_PI = round(max(avg_zscore_price_index, na.rm = TRUE), 1),
min_PI = round(min(avg_zscore_price_index, na.rm = TRUE), 1)) %>% unite(range, min_PI:max_PI, sep = "," )
table1_price_index
table1_case_rate = merged %>% select(county, case_rate, no_cases) %>%
unique()
table1_case_rate
table1_case_rate$combined = paste0(table1_case_rate$case_rate, " (", table1_case_rate$no_cases, ")")
table1_case_rate = table1_case_rate %>% select(county, combined) %>% # note that i took the first occurence of dane, so the higher case rate
arrange(county, desc(combined)) %>% distinct(county, .keep_all=TRUE)
table1_case_rate
## Finally, make a total row for bottom of table, as 1 row dataframe
total = cbind("Wisconsin", "Total", merged %>%
summarize(n = n(), mask_usage = paste0(c(sum(mask),"/",n(), " (", round(sum(mask)/n()*100, 1), ")"), collapse = "")),
merged %>% group_by(gender) %>% summarize(mask_usage = paste0(c(sum(mask), "/",n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = gender, values_from = mask_usage),
merged %>% group_by(age) %>% summarize(mask_usage = paste0(c(sum(mask), "/",n()," (", round(sum(mask)/n()*100, 1), ")"), collapse = "")) %>%
pivot_wider(names_from = age, values_from = mask_usage),
0,0)
ncol(total)
## Setup the final Data Frame
table1 = table1_names %>% left_join(table1_gender, by = "county") %>% left_join(table1_age, by = "county") %>%
left_join(table1_price_index, by = "county") %>% left_join(table1_case_rate, by = "county")
colnames(total) = colnames(table1)
final_table1 = rbind(table1, total) # combine main table with total row
## Write out the Table 1 for Publication
write.table(final_table1, "Demographics/table.txt", sep="\t", quote=F, row.names=F)
# SECTION 3: Logistic Regression and Odds Ratios ####
merged = read.csv("InputData/meta_merged_observed.csv",row.names = 1, check.names = F, stringsAsFactors = 1)
merged$gender = as.factor(recode(merged$gender, `0` = "Male", `1` = "Female"))
merged$gender = fct_relevel(merged$gender, ref = "Male")
#merged$age = recode(merged$age, a = "30-59", c = "<18", y = "18-29", e = "60+")
#merged$age = fct_relevel(merged$age, levels = c("<18", "18-29", "30-59", "60+"))
merged$age = fct_relevel(merged$age, levels = c("c", "y", "a", "e"))
merged$age
merged$age = droplevels(merged$age)
merged$age
# Setting threshold for high case rate vs low case rate
threshold = 800
plot(density(merged$case_rate))
abline(v=threshold,col="red")
test = merged
#top 5, not used
merged_top5 <- merged %>%
filter(county %in% c("dane", "brown", "racine", "kenosha", "milwaukee"))
# merged_nottop5 <- merged %>%
#   filter(!(county %in% c("dane", "brown", "racine", "outagamie", "winnebago")))
merged$pop_total = merged$pop_total/10000
merged$pop_total
model = glm(mask ~ age + avg_zscore_price_index + gender +case_rate*pop_total, data = merged, family = binomial)
summary(model)
# Converting logistic regression coef. into adjusted OR
OR = data.frame(exp(cbind("Odds ratio" = coef(model), confint.default(model, level = 0.95))), pvalue = summary(model)$coefficients[,4], check.names = F)
OR
# Remove the intercept row
abbreviated_1 = OR[-1,1]
abbreviated_2 = OR[-1,2]
abbreviated_3 = OR[-1,3]
abbreviated_4 = OR[-1,4]
# SECTION 4: Plot figure 1B for aOR ####
tabletext=cbind(
c(" ", "Young adult", "Adult", "Older adult", "High price index", "Female gender", "High case prevalence/10K", "Large population/10K", "Case prevalence:population"),
c("aOR", formatC(abbreviated_1, digits = 2, drop0trailing = FALSE, format = "f")),
c("Lower CI", formatC(abbreviated_2, digits = 2, format = "f", drop0trailing = FALSE)),
c("Upper CI", formatC(abbreviated_3, digits = 2, format = "f", drop0trailing = FALSE)),
c("P-value", formatC(abbreviated_4, format = "e", digits = 2)))
tabletext
png("forestplot.png", width = 2600, height = 980)
forestplot(labeltext = tabletext,
mean = c(NA, abbreviated_1),
lower = c(NA, abbreviated_2),
upper = c(NA, abbreviated_3),
ci.vertices = T,
is.summary = c(TRUE, rep(FALSE, 9)),
fn.ci_norm = fpDrawCircleCI,
boxsize = 0.3,
txt_gp = fpTxtGp(ticks = gpar(cex = 3.6), xlab = gpar(cex = 3.6 ), label = gpar(cex =3.6)),
xlab = " ",
lwd.ci = 6,
xlog = TRUE,
#xticks = c(0, 1, 2, 3, 4, 5, 6),
hrzl_lines = gpar(col = "#444444"),
linesheight = "lines",
new_page = FALSE,
col = fpColors(box = c("black"), lines = "black"))
dev.off()
#pull state boundaries
states= map_data("state")
wisconsin=subset(states,region =='wisconsin')
#add in county lines
#column headers are: long,lat,group,order,region,subregion
usa_counties=map_data('county')
counties=subset(usa_counties,region=='wisconsin')
counties$subregion = replace(counties$subregion, counties$subregion=="st croix", "st_croix")
counties$subregion = replace(counties$subregion, counties$subregion=="fond du lac", "fond_du_lac")
observation_data = read.csv("./InputData/meta_merged_observed.csv", stringsAsFactors = FALSE) %>%
group_by(county) %>%
summarize(`Face Covering Use (%)` = sum(mask, na.rm = TRUE)/n()*100, `COVID-19 Prevalence \n(per 100K)` = mean(case_rate, na.rm = TRUE)) %>%
unique() %>%
left_join(counties, by = c("county" = "subregion")) %>% unique()
#pull all rows with subregion of adams,brown,dane,grant,eau claire,green,iowa,
#jackson,kenosha,lafayette,milwaukee,monroe, outagamie,st croix,pierce,polk,
#rock, walworth, wood
observed_counties=observation_data%>%
filter(county %in% c('adams','brown','dane','grant','iowa',
'jackson','kenosha','lafayette','milwaukee',
'monroe','outagamie','st_croix','pierce','polk','walworth','wood','racine', 'winnebago', 'waushara','fond_du_lac'))
head(observed_counties) #make sure it has all the columns still
library(RColorBrewer)
library(maps)
library(mapdata)
library(ggplot2)
library(tidyverse)
#blank WI with no gridlines in background and fixed coordinates so increasing
#figure wont change dimensions
WIoutline = ggplot(data = wisconsin, mapping = aes(x = long, y = lat, group = group)) +
coord_fixed(1.4) + geom_polygon(fill = "white", color = "black", size = 0.01)
#add in the county lines with black outlines and safe as WIcounty
WIoutline
WIcounty = WIoutline+ theme_nothing(legend = TRUE) + geom_polygon(data=counties, fill = NA, color='black', size=0.05)+
geom_polygon(color='black', fill=NA, size = 0.05)
WIcounty
WImask = WIcounty + geom_polygon(aes(fill = `Face Covering Use (%)`), data = observed_counties) +
scale_fill_distiller(palette = "Greys", direction = 1)
WImask
# custom name for COVID Prevalence per 100,000
observed_counties = observed_counties %>% rename(`COVID-19 Prevalence \n(per 10,000)` = `COVID-19 Prevalence \n(per 100K)`)
#add in counties that we observed at outlined in red
WIobserved = WImask + geom_polygon(aes(color = `COVID-19 Prevalence \n(per 10,000)`), data=observed_counties,
fill=NA, size = 1) + scale_color_distiller(palette = "RdYlBu")
#geom_polygon(color='black',fill=NA)
WIobserved + theme(legend.key.size = unit(.75, "cm"),
legend.title = element_text(size = 16),
legend.text = element_text(size = 14))
ggsave("Fig1A.png", width = 2400, height = 980)
######### RELATIVE RISK June 18 TN, **** NOT USED **** #####
View(merged)
#geom_polygon(color='black',fill=NA)
WIobserved + theme(legend.key.size = unit(.75, "cm"),
legend.title = element_text(size = 16),
legend.text = element_text(size = 14))
# Setup packages ####
### Install packages only if not present
if (!require("ggpubr")) install.packages("ggpubr")
if (!require("tidyverse")) install.packages("tidyverse")
### Loading packages
library(tidyverse); library(ggpubr)
### Setting working directory for Github repo
setwd('~/Documents/GitHub/ProjectSnafu')
# SECTION 1: RETROSPECTIVE: Assessing representativeness of sample #####
census_income <- read.csv("InputData/ACSST5Y2018.S1903_data_with_overlays_2020-05-29T093325.csv",
skip = 1, stringsAsFactors = FALSE)
#View(census_income)
census_income$Estimate..Median.income..dollars...FAMILIES..Families <- as.numeric(census_income$Estimate..Median.income..dollars...FAMILIES..Families)
census_population <- read.csv("InputData/ACSST5Y2018.S0101_data_with_overlays_2020-05-29T133200.csv", stringsAsFactors = FALSE,
skip = 1)
census_race <- read.csv("InputData/ACSDP5Y2018.DP05_data_with_overlays_2020-05-29T092432.csv",
skip = 1, stringsAsFactors = FALSE)
View(census_race)
### Setting working directory for Github repo
setwd('~/Documents/GitHub/ProjectSnafu')
### Setting working directory for Github repo
setwd('~/Documents/GitHub/ProjectSnafu')
# SECTION 1: RETROSPECTIVE: Assessing representativeness of sample #####
census_income <- read.csv("InputData/ACSST5Y2018.S1903_data_with_overlays_2020-05-29T093325.csv",
skip = 1, stringsAsFactors = FALSE)
#View(census_income)
census_income$Estimate..Median.income..dollars...FAMILIES..Families <- as.numeric(census_income$Estimate..Median.income..dollars...FAMILIES..Families)
census_population <- read.csv("InputData/ACSST5Y2018.S0101_data_with_overlays_2020-05-29T133200.csv", stringsAsFactors = FALSE,
skip = 1)
census_race <- read.csv("InputData/ACSDP5Y2018.DP05_data_with_overlays_2020-05-29T092432.csv",
skip = 1, stringsAsFactors = FALSE)
View(census_race)
# Read in Meta data
geo_id_data <- read.csv("InputData/20-0708-ppe_meta_data.csv") %>% select(id, geo_id) %>% unique()
# Read in Observational data
ppe_meta <- read.csv("InputData/meta_merged_observed.csv", stringsAsFactors = FALSE) %>%
left_join(geo_id_data, by = "id") %>%
filter(type == "grocery")
ppe_meta <- ppe_meta %>%
mutate(tag = "1400000US")%>%
unite(complete_id, c(tag, geo_id), sep = "", remove = FALSE)
ppe_id_match <- ppe_meta %>% select(id, complete_id) %>% unique()
ppe_obs <- read.csv("InputData/meta_merged_observed.csv", stringsAsFactors = FALSE) %>%
left_join(ppe_id_match, by = "id") %>%
group_by(complete_id) %>%
summarize(n = n())
##Calculating adjusted income for all census tracts in Wisconsin. Adjusted income =  Median family income for census tract divided by total number of people in census tract
all <- census_income %>% left_join(census_population, by = "id") %>%
mutate(adj_median_income = Estimate..Median.income..dollars...FAMILIES..Families,
adj_median_income_scaled = Estimate..Median.income..dollars...FAMILIES..Families/Estimate..Total..Total.population) %>%
select(id, adj_median_income, Estimate..Total..Total.population, adj_median_income_scaled)
View(all)
##Calculating adjusted income for census tracts we have sampled.
stuff <- ppe_obs %>% left_join(census_income, by = c("complete_id" = "id")) %>%
left_join(census_population, by = c("complete_id" = "id")) %>%
mutate(adj_median_income = Estimate..Median.income..dollars...FAMILIES..Families,
adj_median_income_scaled = Estimate..Median.income..dollars...FAMILIES..Families/Estimate..Total..Total.population,
zscore = (Estimate..Median.income..dollars...FAMILIES..Families-mean(all$adj_median_income, na.rm = TRUE)) / sd(all$adj_median_income, na.rm = TRUE)) %>%
left_join(ppe_meta, by = "complete_id") %>%
select(complete_id, adj_median_income, name, county) %>% unique()
a <- stuff %>% ggplot(aes(x = adj_median_income)) + geom_histogram() + xlim(0, 200000) + ggtitle("Our collection")
b <- all %>% ggplot(aes(x =adj_median_income)) + geom_histogram() + xlim(0, 200000) + ggtitle("What WI looks like")
ggarrange(a, b)
ggsave("income.png")
# Calculating percentage of individuals identifiying as white in each census tract. Adjusted for number of individuals residing in each census tract.
all_race <- census_race %>%
mutate(percent_white = (Estimate..RACE..Total.population..One.race..White / Estimate..RACE..Total.population * 100)) %>%
select(id, percent_white, Estimate..RACE..Total.population)
##Calculating adjusted percent white for census tracts we have sampled.
stuff_race <- ppe_obs %>% left_join(census_race, by = c("complete_id" = "id")) %>%
mutate(percent_white = Estimate..RACE..Total.population..One.race..White / Estimate..RACE..Total.population * 100)
c <- stuff_race %>% ggplot(aes(x = percent_white)) + geom_histogram() + xlim(0, 100) + ggtitle("Our collection")
d <- all_race %>% ggplot(aes(x = percent_white)) + geom_histogram() + xlim(0, 100) + ggtitle("What WI looks like")
ggarrange(c, d)
ggsave("percent_white.png")
# KS tests to determine if the two distributions are similar.
percent_white = ks.test(all_race$percent_white, stuff_race$percent_white)
income = ks.test(all$adj_median_income, stuff$adj_median_income)
# Check, these are public demographic data
sum(complete.cases(all_race$percent_white))/length(all_race$percent_white) # 98.8% data complete
sum(complete.cases(all$adj_median_income))/length(all$adj_median_income) # 98.1% data complete
View(all)
# Check, these are OUR demographic data
sum(complete.cases(stuff_race$percent_white))/length(stuff_race$percent_white) # 84% data complete, race
sum(complete.cases(stuff$adj_median_income))/length(stuff$adj_median_income) # 84.6% data complete, income
# Check missing data, these are public data
stuff[not(complete.cases(stuff$adj_median_income)),]
stuff[not(complete.cases(stuff$adj_median_income)),]$complete_id
stuff_race[not(complete.cases(stuff_race$percent_white)),]$complete_id
# Plot the two densities together
plot_df = data.frame(Category="Our Data", Var=stuff$adj_median_income, check.names=F)
plot_df = rbind(plot_df, data.frame(Category="WI", Var=all$adj_median_income, check.names=F))
i = ggplot(plot_df, aes(x = Var, color = Category)) + geom_density() + ggtitle("Median Income") + labs(
title = "Median Income",
subtitle = paste0("KS p val = ", round(percent_white$p.value,3))
) + theme_classic() + theme(legend.position = "none") + theme(
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("Median Income per Census Tract") + ylab("Density")
plot_df = data.frame(Category="Our Data", Var=stuff_race$percent_white, check.names=F)
plot_df = rbind(plot_df, data.frame(Category="WI", Var=all_race$percent_white, check.names=F))
r = ggplot(plot_df, aes(x = Var, color = Category)) + geom_density() + labs(
title = "Race",
subtitle = paste0("KS p val = ", round(income$p.value,3))
) + theme_classic() + theme(
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) + xlab("% Non-White per Census Tract")
ggarrange(i,r)
ggsave("combined_KS.png",
width = 8,
height = 5)
percent_white
income
